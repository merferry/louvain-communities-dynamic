We now describe the computational artifact for the paper \textbf{Shared-Memory Parallel} \textbf{Algorithms for Community Detection in Dynamic Graphs}. It includes the source code for \textit{three experiments} and the source code for \textit{generating plots} in four separate directories.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
% \tightlist
\item
  \texttt{louvain-communities-openmp-dynamic/} contains the source code for the experiment which compares the performance of \textit{Static}, \textit{Naive-dynamic}, \textit{Dynamic Delta-screening}, and \textit{Dynamic Frontier} based \textit{Louvain}.
\item
  \texttt{rak-communities-openmp-dynamic/} contains the source code for the experiment which compares the performance of \textit{Static}, \textit{Naive-dynamic}, \textit{Dynamic} \textit{Delta-screening}, and \textit{Dynamic Frontier} based \textit{LPA} (aka \textit{RAK}).
\item
  \texttt{communities-openmp-dynamic/} contains the source code for the experiment which compares the performance of \textit{Dynamic Frontier} based \textit{Louvain}, \textit{LPA}, and \textit{Hybrid Louvain-LPA}. It also includes the script (and steps) to run the strong scaling experiment.
\item
  \texttt{gnuplot-scripts-communities-cpu/} contains the source code for generating the plots for the experiments.
\end{enumerate}




\subsection{Dependencies and requirements}

We run all experiments on a server that has an AMD EPYC-7742 64-bit processor. This processor has a clock frequency of 2.25 GHz and 512 GB of DDR4 system memory. The CPU has 64 x86 cores. Each core has L1 cache of 4 MB, L2 cache of 32 MB, and a shared L3 cache of 256 MB. The machine runs on \textit{Ubuntu 20.04}. It is possible to run the experiment on any 64-bit system running a recent version of Linux by configuring the number of threads to use for the experiment. We use \textit{GCC 9.4} and \textit{OpenMP 5.0} to compile with optimization level 3 (\texttt{-O3}). Executing the build and run script requires \texttt{bash}. Additionally, \textit{Node.js 18 LTS} is needed to process generated output into \textit{CSV}, \textit{Google} \textit{sheets} is needed to generate charts and summarized CSVs, and \textit{gnuplot 5.4} is needed to generate the plot from summarized CSVs.

We use \texttt{13} graphs in \textit{Matrix Market (.mtx)} file format from the \textit{SuiteSparse Matrix Collection} as our input dataset. These must be placed in the \texttt{\textasciitilde{}/Data} directory \textbf{before running} the experiments. In addition, a \texttt{\textasciitilde{}/Logs} directory \textbf{must be created}, where the output logs of each experiment are written to. Please use \texttt{setup.sh} in the current directory to create the necessary directories and download the input dataset. The graphs in the \textbf{input dataset} are as follows:

\begin{verbatim}
indochina-2004.mtx
uk-2002.mtx
arabic-2005.mtx
uk-2005.mtx
webbase-2001.mtx
it-2004.mtx
sk-2005.mtx
com-LiveJournal.mtx
com-Orkut.mtx
asia_osm.mtx
europe_osm.mtx
kmer_A2a.mtx
kmer_V1r.mtx
\end{verbatim}




\subsection{Installation and deployment process}

Each experiment includes a \texttt{mains.sh} file which needs to be \textbf{executed} in order to run the experiment. To run an experiment, try the following:

\begin{verbatim}
# Run experiment with a default of 64 threads
$ DOWNLOAD=0 ./mains.sh

# Run experiment with 32 threads
$ DOWNLOAD=0 MAX_THREADS=32 ./mains.sh
\end{verbatim}

Please refer to any additional details in the \texttt{README.md} of each experiment. Output logs are written to the \texttt{\textasciitilde{}/Logs} directory. These logs can be processed with the \texttt{process.js} script to generate a \textit{CSV} file as follows:

\begin{verbatim}
$ node process.js csv ~/Logs/"experiment".log "experiment".csv
\end{verbatim}

The generated \textit{CSV} file can be loaded into the \texttt{data} sheet of the linked \textbf{sheets} document in the respective experiment. Ensure that there are no newlines at the end of the \texttt{data} sheet after loading. All the charts are then automatically updated. See \texttt{"graph"} sheet for results on a specific input graph, or the \texttt{all} sheet for the average result on all input graphs. You can then use the \texttt{csv} sheet to retrieve \textit{summarized CSVs} which can be used to generate plots using the \texttt{gnuplot} scripts in the \texttt{gnuplot-scripts-communities-cpu} directory.




\subsection{Reproducibility of Experiments}

The workflow of each experiment is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
% \tightlist
\item
  Setup the necessary directories and download the input dataset with \texttt{setup.sh}.
\item
  Run an experiment of choice with \texttt{DOWNLOAD=0\ ./mains.sh} in respective subdirectory.
\item
  Output of the experiment is written to \texttt{\textasciitilde{}/Logs} directory.
\item
  Process the output logs into CSV with \texttt{node\ process.js\ csv\ \textasciitilde{}/Logs/"experiment".log\ "experiment".csv}.
\item
  Import the CSV into the \texttt{data} sheet of the linked \textbf{sheets} document of the experiment.
\item
  All the charts are automatically updated. See \texttt{"graph"} sheet for results on a specific input graph, or the \texttt{all} sheet for the average result on all input graphs.
\item
  Use the \texttt{csv} sheet to retrieve summarized CSVs.
\item
  Use the summarized CSVs to generate plots using the \texttt{gnuplot} scripts in the \texttt{gnuplot-scripts-communities-cpu} subdirectory.
\item
  Compare the generated plots with that of the paper.
\end{enumerate}
